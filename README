LINUX FOUNDATION BUILDBOT CONFIG
================================

This information should help people understand and work with the Linux
Foundation's buildbot setup.

Contents
--------

The current buildbot settings and scripts are stored here.  There are
currently four types of files here:

 - lsb_master.cfg and moblin_master.cfg, the master configuration for
   buildbot

 - support Python modules for the above

 - master-side support scripts, that do things like assemble the
   snapshots directory and build repositories

Setup
-----

Setting up the buildbot master is now done via Puppet.  In the LSB's
Puppet configuration, add "include buildbot::master" to the node's
definition.

Slave setup
-----------

Slave setup is transitioning to Puppet as well.  To add a slave via
Puppet, simply add "include buildbot::slave" to that node's Puppet
configuration.  Once Puppet has had a chance to set up the slave, be
sure and trigger "build-sdk" and "libbat" for that slave's
architecture; this will ensure that all the necessary dependencies for
proper builds are present.

The following procedure documents how to add a build slave on a
non-Puppet machine, but for best results, use Puppet to deploy the
build slave setup.

While the masters are expected to be more-or-less permanent, the
slaves are completely replaceable.  Here's the procedure for setting
up a new slave:

1.  Install the tools needed for building all projects in the SDK on
the slaves.  At minimum, that will include bzr and the base
development tools.

2.  Create a "buildbot" user.  Make sure the account is locked however
that works for the distribution being used.

3.  Install buildbot on each slave.  This will likely require
installing Twisted.  Be sure to install the core, Mail, Web, and
Words.

4.  Create a slave instance according to the buildbot documentation as
the buildbot user.  Be sure and pass the argument "--umask=022" to the
create-slave command, so the permissions on the resulting packages
aren't too restrictive.

5.  Give password-less access via sudo to the rpm command for the
buildbot user.  The builds will require the buildbot to install and
remove packages.  Something like this should do:

buildbot ALL=NOPASSWD: /bin/rpm

6.  Copy the scripts from the puppet-lsb project, under
"modules/buildbot/files/slavescripts", into a directory on the default
system path.

7.  Download a copy of the current released LSB SDK bundle tarball to
the slave, and symlink it to $HOME/lsb-released-sdk.tar.gz (for the
buildbot user).

8.  If there's a beta in progress, download a copy of the current beta
SDK bundle tarball to the slave, and symlink it to
$HOME/lsb-beta-sdk.tar.gz (again, for the buildbot user).  If this is
not done, beta builds may not work.  As an alternative, if the SDK
itself is not in beta, symlink the production SDK to the beta
filename.

9.  Add the command to start the slave on reboot.  If you installed
buildbot on the slave via a distro package, this may be set up
already; see your distro's buildbot docs.  If not, a line like this in
buildbot's crontab should do the trick:

@reboot buildbot start /path/to/slave

10. Start the slave.

11. Force a build of the "build-sdk" and "libbat" jobs for that
architecture.  This ensures that all build dependencies are present on
the slave.

Using the configuration
-----------------------

Both masters include an IRC bot and a Web status page that can be used
to trigger builds.  See the buildbot documentation for details.

On the LSB side, there is a separate build for each architecture; they
are named for their version control repository and architecture,
separated by a dash.  So, for example, the ia64 build of our Python
tests is called "python-test-ia64".  For stuff built out of
"packaging", the subdir name from "packaging" is used.  The SDK is
built as a single build, called "build-sdk" (plus the arch), and the
appbat is split into two builds, called "libbat" and "appbat".

The LSB side is also special because we have different types of
builds: "production", "development", and "beta".  Depending on the
type of build, different versions of the SDK are used to build, and
package versions may get embedded dates.

By default, all builds are "production" builds unless done from the
devel tree, in which case they default to "development".  The build
type can be forced by supplying a reason for the build which starts
with either "production:" or "beta:".  (It's assumed that development
builds of released products aren't needed.)  This ultimately
translates into a "build_type" property on the build, which drives the
other processes in the build.

MultiScheduler
--------------

Because the LSB builder has so many builds, all related in certain
ways, we provide a system for starting multiple builds at once with a
single action.  Currently, this takes the form of a spooler directory,
which is watched for spool files describing a build request.

The easiest way to take advantage of the MultiSchedule is with the
provided command-line utility, "start_lsb_build".  This utility can
create properly-formatted job files using simple commands.  To learn
how to use start_lsb_build, run it with the -h parameter, which will
print a short help message.  The utility can auto-submit the job (and
does so by default), or it can be used to create job files that can be
submitted later by copying them to the spool directory.

There are some cases where direct manipulation of spool files is
necessary; for example, when submitting jobs automatically in slightly
different ways depending on the context.  The files consist of
multiple lines of property=value pairs.  Properties recognized:

 branch_name - Name of the bzr branch directory containing the projects
 in question.  For example, if a project to build is found at
 "http://bzr.linuxfoundation.org/lsb/devel/build_env", the branch_name
 for this build would be "devel".

 projects - Comma-separated list of projects to build.  The project
 name consists of the builder name in buildbot minus the trailing
 architecture identifier.  For example, if one of the buildbot builders
 to be triggered is "azov-qt3-tests-x86_64", the project would be
 "azov-qt3-tests".

 architectures - Comma-separated list of architectures to build for.
 Using the previous example for "projects", the architecture would be
 "x86_64".

 tag - the bzr tag name as passed to the "-r" parameter of bzr, minus
 the "tag:" specifier.

 build_type - the type of build being done.  This must be "normal",
 "production", or "beta".

A minimal request file must contain the "projects" and "branch_name"
properties.  The rest of the properties default to a normal build, for
all architectures.
